# ğŸ“˜ Glossary and ML Workflow Cheatsheet

This file explains **all jargon and technical terms**, plus a **step-by-step ML workflow**, **common issues**, and **tips to solve them**.

---

## ğŸ“Š **Key Concepts in Machine Learning**

### ğŸ”¹ **Dataset**

A collection of features (input variables) and target (output variable).

### ğŸ”¹ **Feature (X)**

The input variables used to predict the target.

### ğŸ”¹ **Target (y)**

The output variable we are trying to predict (classification or regression).

### ğŸ”¹ **Train-Test Split**

Dividing the dataset into **training data** (to train the model) and **test data** (to evaluate performance).

### ğŸ”¹ **Stratified Train-Test Split**

Ensures that the proportion of classes in training and test sets remains the same as the original dataset.

### ğŸ”¹ **Cross-Validation (CV)**

Splits the dataset into multiple folds for training and validation to get more reliable performance estimates.

### ğŸ”¹ **KFold**

Cross-validation method that splits the data into K equally sized folds.

### ğŸ”¹ **StratifiedKFold**

A variant of KFold for classification problems that preserves class proportions in each fold.

---

## ğŸ¤– **Machine Learning Algorithms Used**

### ğŸ”¹ **Logistic Regression**

A simple classification algorithm that predicts the probability of classes using a logistic (sigmoid) function.

### ğŸ”¹ **Random Forest**

An ensemble algorithm that combines multiple decision trees and averages their predictions for better accuracy.

### ğŸ”¹ **Linear Regression**

A regression algorithm that fits a straight line to predict continuous values.

### ğŸ”¹ **Ridge Regression**

A variant of linear regression with regularization to prevent overfitting.

### ğŸ”¹ **KMeans Clustering**

An unsupervised algorithm that partitions data into **k clusters** by minimizing intra-cluster distance.

### ğŸ”¹ **Agglomerative Clustering**

A hierarchical clustering algorithm that merges clusters iteratively based on similarity.

### ğŸ”¹ **PCA (Principal Component Analysis)**

A dimensionality reduction technique that projects high-dimensional data into fewer dimensions while preserving variance.

---

## ğŸ“ˆ **Metrics Used**

### ğŸ”¹ **Accuracy**

Percentage of correctly predicted samples:
$$ \text{Accuracy} = \frac{\text{Correct Predictions}}{\text{Total Samples}} $$

### ğŸ”¹ **Precision**

Out of all predicted positives, how many are correct:
$$ \text{Precision} = \frac{TP}{TP+FP} $$

### ğŸ”¹ **Recall (Sensitivity)**

Out of all actual positives, how many are correctly predicted:
$$ \text{Recall} = \frac{TP}{TP+FN} $$

### ğŸ”¹ **F1-Score**

Harmonic mean of Precision and Recall:
$$ F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall} $$

### ğŸ”¹ **ROC-AUC Score**

Measures how well the model separates classes by plotting True Positive Rate vs False Positive Rate.

### ğŸ”¹ **RÂ² Score**

Explains how much variance in the target variable is explained by the model.

### ğŸ”¹ **RMSE (Root Mean Squared Error)**

Measures the standard deviation of prediction errors:
$$ RMSE = \sqrt{\frac{1}{n}\sum(y_{true} - y_{pred})^2} $$

### ğŸ”¹ **MAE (Mean Absolute Error)**

Average of absolute errors:
$$ MAE = \frac{1}{n}\sum|y_{true} - y_{pred}| $$

### ğŸ”¹ **Silhouette Score**

Measures how well samples are clustered:

- Close to **1** â†’ good clustering
- Close to **0** â†’ overlapping clusters

### ğŸ”¹ **Inertia (Elbow Method)**

Sum of squared distances of samples to their nearest cluster center. Used to find optimal number of clusters.

---

## ğŸ“¦ **Libraries Used**

### ğŸ”¹ **pandas** â†’ Data handling and manipulation

### ğŸ”¹ **numpy** â†’ Numerical operations

### ğŸ”¹ **matplotlib / seaborn** â†’ Visualization

### ğŸ”¹ **scikit-learn (sklearn)** â†’ ML algorithms, datasets, metrics, model selection utilities

---

## ğŸ”‘ **Important Functions Used**

| Function | Purpose |
|-----------|---------|
| `load_breast_cancer()` | Loads breast cancer dataset |
| `load_diabetes()` | Loads diabetes dataset |
| `load_iris()` | Loads iris dataset |
| `train_test_split()` | Splits dataset into train and test |
| `StratifiedKFold()` | Cross-validation for classification |
| `KFold()` | Cross-validation for regression |
| `cross_val_score()` | Computes CV score for a model |
| `fit()` | Trains the model |
| `predict()` | Makes predictions on data |
| `classification_report()` | Shows precision, recall, F1-score |
| `accuracy_score()` | Computes accuracy |
| `roc_auc_score()` | Computes ROC-AUC score |
| `mean_squared_error()` | Computes MSE for regression |
| `r2_score()` | Computes RÂ² score |
| `PCA()` | Reduces data to fewer dimensions |
| `KMeans()` | Performs k-means clustering |
| `AgglomerativeClustering()` | Performs hierarchical clustering |
| `silhouette_score()` | Evaluates clustering quality |

---

## ğŸ¯ **How to Use This Glossary?**

Use this file while running the scripts to **understand every method, metric, and ML concept** step by step.

---

## ğŸ”„ **Overall Machine Learning Cycle**

1ï¸âƒ£ **Understand the Problem**  
   â€“ Is it **classification, regression, or clustering**?  
   â€“ Identify input (features) and output (target).

2ï¸âƒ£ **Data Collection / Loading**  
   â€“ Load built-in datasets (e.g., `load_breast_cancer()`, `load_diabetes()`).

3ï¸âƒ£ **Exploratory Data Analysis (EDA)**  
   â€“ Check shape, missing values, distributions, correlations.  
   â€“ Visualize data to spot patterns or class imbalance.

4ï¸âƒ£ **Data Preprocessing**  
   â€“ Handle missing values, scaling, encoding (if needed).  
   â€“ Feature selection/dimensionality reduction (PCA).

5ï¸âƒ£ **Split Data**  
   â€“ Use `train_test_split()` (Stratified for classification).

6ï¸âƒ£ **Model Selection**  
   â€“ Start with simple models (Logistic/Linear Regression).  
   â€“ Try advanced models (RandomForest, Ridge, etc.).

7ï¸âƒ£ **Validation**  
   â€“ Use `cross_val_score()` with **KFold/StratifiedKFold**.  
   â€“ Compare metrics (Accuracy, RÂ², RMSE, etc.).

8ï¸âƒ£ **Prediction & Evaluation**  
   â€“ Check test performance.  
   â€“ Use confusion matrix, ROC-AUC (for classification).

9ï¸âƒ£ **Model Improvement**  
   â€“ Hyperparameter tuning.  
   â€“ Try different algorithms, more data, feature engineering.

10ï¸âƒ£ **Deployment or Reporting**  
   â€“ Save model, share predictions, or integrate into apps.

---

## âš ï¸ **Common Problems and Solutions**

### ğŸ“Œ **1. Poor Validation Accuracy**

âœ… Causes:

- Overfitting to training data.
- Class imbalance.
- Irrelevant or redundant features.

âœ… Solutions:

- Use **cross-validation (KFold)** to get stable scores.
- Perform **feature selection** (drop highly correlated features).
- Collect more data or add regularization (e.g., Ridge, Lasso).

---

### ğŸ“Œ **2. Class Imbalance in Classification**

âœ… Causes:

- One class dominates dataset (e.g., 90% negative, 10% positive).

âœ… Solutions:

- Use **Stratified Train-Test Split**.
- Evaluate with **Precision, Recall, F1-score**, not just accuracy.
- Use oversampling (SMOTE) or class weights in models.

---

### ğŸ“Œ **3. Overfitting (High Train Accuracy, Low Test Accuracy)**

âœ… Causes:

- Model too complex (e.g., deep tree).
- Too many features vs. samples.

âœ… Solutions:

- Use simpler models or regularization (Ridge/Lasso).
- Try **cross-validation**.
- Get more training data.

---

### ğŸ“Œ **4. Underfitting (Low Accuracy in Both Train and Test)**

âœ… Causes:

- Model too simple (e.g., Linear Regression for complex data).

âœ… Solutions:

- Try more powerful models (RandomForest, Gradient Boosting).
- Add interaction features or polynomial terms.

---

### ğŸ“Œ **5. Clustering Issues (Wrong Number of Clusters)**

âœ… Causes:

- Choosing wrong `k` in KMeans.

âœ… Solutions:

- Use **Elbow Method (Inertia)** to find optimal k.
- Check **Silhouette Score** for cluster quality.

---

## ğŸ¯ **Key Tips for Beginners**

ğŸ”¹ Always **start with EDA** â€“ understand your data before training.  
ğŸ”¹ **Use CV (KFold/StratifiedKFold)** to avoid misleading single-split results.  
ğŸ”¹ Compare **multiple metrics** (Accuracy, F1, ROC-AUC for classification; RÂ², RMSE for regression).  
ğŸ”¹ For clustering, always **visualize clusters (PCA, scatter plots)**.  
ğŸ”¹ Donâ€™t rely on just one model â€“ **try different algorithms**.

---

The first part of this file still contains **Glossary definitions** for all terms used in code. The second part is this **ML Workflow Cheatsheet** to guide you step by step.
